// (local-set-key (kbd "s-r") (lambda () (interactive) (save-buffer) (process-send-string "*shell skm-oed*" "echo '\n'; webppl vi-bern-amortized.wppl\n")))

// show and quit
var SNQ = function(v) {
  display(v)
  process.exit()
}


var hack = {
  log: _.throttle(console.log, 300)
}


var model = function() {
  var nh = globalStore['nh'], nt = globalStore['nt']
  var weight = sample(Beta({a: 2, b: 2}),
                      {guide: function() {
                        var net = affine('recog', {in: 2, out: 2})
                        var out = net(Vector([nh, nt]))
                        // sqrt(x^2 + eps^2) is a differentiable approximation to abs
                        var outPos = T.sqrt(T.add(T.pow(out, 2), T.mul(Vector([1,1]), 0.01)))

                        //var outPos = T.abs(out)
                        // var y = T.sigmoid(net(Vector([nh, nt])));
                        // // squish to (2,b)
                        // var a = 2
                        // var b = 50
                        // var params = T.add(T.mul(y, b - a), a);
                        //hack.log(T.toScalars(ad.valueRec(params)))
                        //display(T.toScalars(y))

                        // display(nh + ',' + nt)
                        // display(T.toScalars(params))
                        // display('')
                        //display(T.toScalars(outPos))

                        Beta({a: T.get(outPos, 0), b: T.get(outPos, 1)})

                        // var guideMu = T.get(netOutput, 0)
                        // var guideSigma = Math.exp(T.get(netOutput, 1))
                        // return LogitNormal({a: 0.0001,
                        //                     b: 0.9999,
                        //                     mu: T.get(outPos, 0),
                        //                     sigma: T.get(outPos, 1)})

                      }})
  var score = nh * Math.log(weight) + nt * Math.log(1 - weight);
  factor(score)
  return weight
}

var numELBOSamples = 10;

var TRAIN = {
  iter: 0,
  data: null
}
var _sampleTrainingData = function() {
  return {nh: randomInteger(10) + 1,//randomInteger(30),
          nt: randomInteger(10) + 1}//randomInteger(30)}
}

var trainingBlockSize = 10;
var sampleTrainingData = function() {
  if (TRAIN.iter % (numELBOSamples * trainingBlockSize) == 0) {
    _.extend(TRAIN, {data: _sampleTrainingData()})
  }
  _.extend(TRAIN, {iter: TRAIN.iter + 1})
  return TRAIN.data
}



// SNQ(expectation(Infer({
//   model: function() { globalStore['nh'] = 12;
//                       globalStore['nt'] = 5;
//                       model()},
//   method: 'optimize',
//   steps: 2000,
//   estimator: {ELBO: {samples: 20}},
//   optMethod: {adam: {stepSize: 0.01}},
//   verbose: true
// })))


//var trainingData = repeat(30, sampleTrainingData);

globalStore['trainingIteration'] = 0
var model2 = function() {
  if (_.has(globalStore, 'dont-train')) {
    console.log(globalStore)
  } else {
    var obs = Infer({method: 'forward', model: sampleTrainingData, samples: 1}).sample()
    //console.log(obs)
    globalStore['nh'] = obs.nh
    globalStore['nt'] = obs.nt
  }

  return model()
}

util.seedRNG(2)
var dist = Infer({
  model: model2,
  method: 'optimize',
  steps: 10000,
  estimator: {ELBO: {samples: numELBOSamples}},
  optMethod: {adam: {stepSize: 0.01}},
  verbose: true
})


display(getParams())

var w = getParams().recogw
var b = getParams().recogb

// display(Vector([1,12]))
var nnOutput = T.add(T.dot(w, Vector([12,5])), b)

var y = T.abs(nnOutput)

display(y)


globalStore['dont-train'] = true
globalStore['nh'] = 1
globalStore['nt'] = 12
expectation(Infer({model: model2,
                   method: 'optimize',
                   samples: 10}))

//getParams()
